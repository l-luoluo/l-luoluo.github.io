<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>自动求导 on 心有所向，日复一日，必有精进</title>
    <link>http://localhost:1313/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/</link>
    <description>Recent content from 心有所向，日复一日，必有精进</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    
    <managingEditor>xxx@example.com (落-luo)</managingEditor>
    <webMaster>xxx@example.com (落-luo)</webMaster>
    
    <copyright>本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</copyright>
    
    <lastBuildDate>Wed, 19 Nov 2025 15:00:00 +0800</lastBuildDate>
    
    
    <atom:link href="http://localhost:1313/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/index.xml" rel="self" type="application/rss&#43;xml" />
    

    
    

    <item>
      <title>自动求导-代码</title>
      <link>http://localhost:1313/post/no.29/</link>
      <pubDate>Wed, 19 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.29/</guid>
      <description>
        <![CDATA[<h1>自动求导-代码</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/notebooks/chapter_preliminaries/autograd.slides.html#/">自动求导-代码</a></p>
<h3 id="代码实现">
<a class="header-anchor" href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0"></a>
代码实现
</h3><p>假设我们想对函数$y=2x⊤x$关于列向量$x$求导</p>
<h4 id="1">
<a class="header-anchor" href="#1"></a>
1.
</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span> <span class="c1"># 定义一个x，为4维向量</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tensor([0., 1., 2., 3.])
</span></span></code></pre></div><h4 id="2">
<a class="header-anchor" href="#2"></a>
2.
</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># 开启x的梯度计算</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># 查看x的梯度</span>
</span></span></code></pre></div><h3 id="3">
<a class="header-anchor" href="#3"></a>
3.
</h3><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># 计算y</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tensor(28., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><h4 id="4">
<a class="header-anchor" href="#4"></a>
4.
</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 对y进行反向传播</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># 查看x的梯度</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">==</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span> <span class="c1"># 验证x的梯度是否正确</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tensor([ 0.,  4.,  8., 12.])
</span></span><span class="line"><span class="cl">tensor([True, True, True, True])
</span></span></code></pre></div><h4 id="5">
<a class="header-anchor" href="#5"></a>
5.
</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 计算另外一个x</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1"># 清空x的梯度</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> 
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tensor([1., 1., 1., 1.])
</span></span></code></pre></div><h4 id="6">
<a class="header-anchor" href="#6"></a>
6.
</h4><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> 
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> 
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tensor([0., 2., 4., 6.])
</span></span></code></pre></div>
        
        <hr><p>本文2025-11-19首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-19</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category>
      
    </item>
    
    

    <item>
      <title>自动求导</title>
      <link>http://localhost:1313/post/no.28/</link>
      <pubDate>Wed, 19 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.28/</guid>
      <description>
        <![CDATA[<h1>自动求导</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_7.pdf">自动求导</a></p>
<h4 id="1-向量的链式法则">
<a class="header-anchor" href="#1-%e5%90%91%e9%87%8f%e7%9a%84%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99"></a>
1. 向量的链式法则
</h4><p>向量的链式法则是标量的链式法则在向量上的推广。
它的基本思想是：如果一个函数 $f$ 是向量的函数，而 $g$ 是标量的函数，那么 $f$ 对 $g$ 的导数就是 $f$ 对每个元素的导数与 $g$ 对每个元素的导数的点积。</p>
        
        <hr><p>本文2025-11-19首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-19</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
  </channel>
</rss>
