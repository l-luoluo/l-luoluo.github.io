[
  {
    "objectID": "ac146d62b3045e00e3259237d3cd8bba0a5b7a39",
    "permalink": "/post/no.20/",
    "title": "GoogleNet模型实践——水果分类","content": "\r前言\r这是GoogLeNet的另外一个模型实践，这次我们来做水果分类。 关于训练代码和测试代码，还有模型代码 请参考之前的文章。 这里就绪详细介绍一下模型的搭建过程，只是简单的介绍一下，需要修改的部分参数\n训练代码需要修改的参数\rROOT_TRAIN = r\u0026#39;data\\train\u0026#39; normalize = transforms.Normalize(mean=[0.22890568, 0.19639583, 0.1433638 ], std=[0.09950783, 0.07997292, 0.06596899]) 其中的ROOT_TRAIN是训练集的路径，需要根据自己的数据集路径修改。normalize中的mean和std是根据训练集的图片通过[脚本2：数据集的处理]（https://l-luoluo.github.io/post/no.17/） 进行计算得到的，需要根据自己的数据集修改。\n测试代码需要修改的参数\r1.\rROOT_TRAIN = r\u0026#39;data\\train\u0026#39; normalize = transforms.Normalize(mean=[0.22890568, 0.19639583, 0.1433638 ], std=[0.09950783, 0.07997292, 0.06596899]) 将上述代码的修改复制粘贴到测试代码中即可。\n2.\rclasses = [\u0026#39;猫\u0026#39;, \u0026#39;狗\u0026#39;] # with torch.no_grad(): # for b_x, b_y in test_dataloader: # b_x = b_x.to(device) # b_y = b_y.to(device) # # # 设置模型为验证模型 # model.eval() # output = model(b_x) # pre_lab = torch.argmax(output, dim=1) # result = pre_lab.item() # label = b_y.item() # print(\u0026#34;预测值：\u0026#34;, classes[result], \u0026#34;------\u0026#34;, \u0026#34;真实值：\u0026#34;, classes[label]) 只需要将classes修改为\nclasses = [\u0026#39;苹果\u0026#39;,\u0026#39;香蕉\u0026#39;,\u0026#39;葡萄\u0026#39;, \u0026#39;橙子\u0026#39;, \u0026#39;梨\u0026#39;] 关于代码中注释掉的部分，是用来打印每个测试样本的预测值和真实值的，如果需要的话，可以先将其下的代码注释掉，然后将这部分代码取消注释即可。\n","date": "2025-11-13 15:00:00",
    "updated": "2025-11-13 15:00:00"
  }, 
  {
    "objectID": "d4ae64683d5f53ce5bd7177892263577a48c29bd",
    "permalink": "/post/no.17/",
    "title": "GoogleNet模型实践——猫狗分类.1","content": "\r前言\r这篇文章主要介绍如何使用GoogleNet模型对自己的数据集进行猫狗分类前期准备工作， 详细的训练代码在下一篇文章中介绍\n数据集准备\r划分数据集\rimport os from shutil import copy import random def mkfile(file): if not os.path.exists(file): os.makedirs(file) # 获取data文件夹下所有文件夹名（即需要分类的类名） file_path = \u0026#39;data_cat_dog\u0026#39; flower_class = [cla for cla in os.listdir(file_path)] # 创建 训练集train 文件夹，并由类名在其目录下创建5个子目录 mkfile(\u0026#39;data/train\u0026#39;) for cla in flower_class: mkfile(\u0026#39;data/train/\u0026#39; + cla) # 创建 验证集val 文件夹，并由类名在其目录下创建子目录 mkfile(\u0026#39;data/test\u0026#39;) for cla in flower_class: mkfile(\u0026#39;data/test/\u0026#39; + cla) # 划分比例，训练集 : 测试集 = 9 : 1 split_rate = 0.1 # 遍历所有类别的全部图像并按比例分成训练集和验证集 for cla in flower_class: cla_path = file_path + \u0026#39;/\u0026#39; + cla + \u0026#39;/\u0026#39; # 某一类别的子目录 images = os.listdir(cla_path) # iamges 列表存储了该目录下所有图像的名称 num = len(images) eval_index = random.sample(images, k=int(num * split_rate)) # 从images列表中随机抽取 k 个图像名称 for index, image in enumerate(images): # eval_index 中保存验证集val的图像名称 if image in eval_index: image_path = cla_path + image new_path = \u0026#39;data/test/\u0026#39; + cla copy(image_path, new_path) # 将选中的图像复制到新路径 # 其余的图像保存在训练集train中 else: image_path = cla_path + image new_path = \u0026#39;data/train/\u0026#39; + cla copy(image_path, new_path) print(\u0026#34;\\r[{}] processing [{}/{}]\u0026#34;.format(cla, index + 1, num), end=\u0026#34;\u0026#34;) # processing bar print() print(\u0026#34;processing done!\u0026#34;) file_path:用于指定数据集的路径，需要修改为自己的数据集路径 split_rate:用于指定划分数据集的比例，默认值为0.1，即10%的数据用于验证集 data: 用于存放数据的目录 数据集的处理\rfrom PIL import Image import os import numpy as np # 文件夹路径，包含所有图片文件 folder_path = \u0026#39;data_cat_dog\u0026#39; # 初始化累积变量 total_pixels = 0 sum_normalized_pixel_values = np.zeros(3) # 如果是RGB图像，需要三个通道的均值和方差 # 遍历文件夹中的图片文件 for root, dirs, files in os.walk(folder_path): for filename in files: if filename.endswith((\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.bmp\u0026#39;)): # 可根据实际情况添加其他格式 image_path = os.path.join(root, filename) image = Image.open(image_path) image_array = np.array(image) # 归一化像素值到0-1之间 normalized_image_array = image_array / 255.0 # print(image_path) # print(normalized_image_array.shape) # 累积归一化后的像素值和像素数量 total_pixels += normalized_image_array.size sum_normalized_pixel_values += np.sum(normalized_image_array, axis=(0, 1)) # 计算均值和方差 mean = sum_normalized_pixel_values / total_pixels sum_squared_diff = np.zeros(3) for root, dirs, files in os.walk(folder_path): for filename in files: if filename.endswith((\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.bmp\u0026#39;)): image_path = os.path.join(root, filename) image = Image.open(image_path) image_array = np.array(image) # 归一化像素值到0-1之间 normalized_image_array = image_array / 255.0 # print(normalized_image_array.shape) # print(mean.shape) # print(image_path) try: diff = (normalized_image_array - mean) ** 2 sum_squared_diff += np.sum(diff, axis=(0, 1)) except: print(f\u0026#34;捕获到自定义异常\u0026#34;) # diff = (normalized_image_array - mean) ** 2 # sum_squared_diff += np.sum(diff, axis=(0, 1)) variance = sum_squared_diff / total_pixels print(\u0026#34;Mean:\u0026#34;, mean) print(\u0026#34;Variance:\u0026#34;, variance) 这给脚本主题要负责的是数据集的处理，包括归一化和计算均值方差 而这两个值在后续的模型训练中会被用到\n","date": "2025-11-12 15:00:00",
    "updated": "2025-11-12 15:00:00"
  }, 
  {
    "objectID": "aafd48144e4b2b4447348f7abbbc5d4416d59c7d",
    "permalink": "/post/no.18/",
    "title": "GoogleNet模型实践——猫狗分类.2","content": "\r前言\r这篇文章主要是关于猫狗分类的详细的训练代码,关于模型的代码可以参考GoogleNet模型搭建，其中有详细的代码\n训练代码\r1. 导入必要的库\rimport copy import time import torch from torchvision.datasets import ImageFolder from torchvision import transforms import torch.utils.data as Data import numpy as np import matplotlib.pyplot as plt from model import GoogLeNet,Inception import torch.nn as nn import pandas as pd 2. 数据预处理\r# 定义数据集路径 ROOT_TRAIN = r\u0026amp;#39;data\\train\u0026amp;#39; normalize = transforms.Normalize(mean=[0.162, 0.151, 0.138], std=[0.058, 0.052, 0.047]) # 定义数据集处理方法变量 train_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),normalize]) # 加载数据集 train_data = ImageFolder(root=ROOT_TRAIN, transform=train_transform) train_data, val_data = Data.random_split(train_data, [round(0.8*len(train_data)), round(0.2*len(train_data))]) train_dataloader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers=4) val_dataloader = Data.DataLoader(dataset=val_data, batch_size=64, shuffle=True, num_workers=4) …","date": "2025-11-12 15:00:00",
    "updated": "2025-11-12 15:00:00"
  }, 
  {
    "objectID": "b42f88d5ce6bf12ae5672b7919396df1e05c4c58",
    "permalink": "/post/no.19/",
    "title": "GoogleNet模型实践——猫狗分类.3","content": "\r前言\r在之前的两篇文章中，我们分别介绍了GoogleNet数据集的准备和GoogleNet模型的搭建。 这篇文章主要是测试代码的搭建\n测试代码\r1. 导入必要的库\rimport torch import torch.utils.data as Data from torchvision import transforms from torchvision.datasets import ImageFolder from model import GoogLeNet,Inception from PIL import Image 2.数据集的预处理\r# 定义数据预处理 def test_data_process(): # 定义数据集路径 ROOT_TRAIN = r\u0026amp;#39;data\\test\u0026amp;#39; normalize = transforms.Normalize(mean=[0.162, 0.151, 0.138], std=[0.058, 0.052, 0.047]) # 定义数据集处理方法变量 test_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), normalize]) # 加载数据集 test_data = ImageFolder(root=ROOT_TRAIN, transform=test_transform) test_dataloader = Data.DataLoader(dataset=test_data, batch_size=1, shuffle=True, num_workers=0) return test_dataloader 3. 模型的测试代码\rdef test_model_process(model, test_dataloader): # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU device = \u0026amp;#34;cuda\u0026amp;#34; if torch.cuda.is_available() else \u0026amp;#39;cpu\u0026amp;#39; # 讲模型放入到训练设备中 model = model.to(device) # 初始化参数 test_corrects = 0.0 test_num = 0 #  …","date": "2025-11-12 15:00:00",
    "updated": "2025-11-12 15:00:00"
  }, 
  {
    "objectID": "64d0ad85954400c90e6d7778b974302c437cd336",
    "permalink": "/post/no.16/",
    "title": "GoogleNet模型搭建","content": "\r前言\r关于训练代码，可以看之前LeNet-5模型的训练代码，GoogleNet模型的训练代码与LeNet模型的训练代码基本相同，只是模型不同。 详细的训练代码可以参考训练代码和测试代码。 文章链接中有详细的代码\n模型搭建\r1.导入必要的库\rimport torch from torch import nn from torchsummary import summary 2. Inception模块\rclass Inception(nn.Module): def __init__(self, in_channels,c1,c2,c3,c4): super(Inception, self).__init__() self.ReLU = nn.ReLU() # 路线1 1*1卷积 self.p1_1 = nn.Conv2d(in_channels=in_channels, out_channels=c1, kernel_size=1) # 路线二 1*1卷积 3*3卷积 self.p2_1 = nn.Conv2d(in_channels=in_channels, out_channels=c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(in_channels=c2[0], out_channels=c2[1], kernel_size=3,padding=1) # 路线3 1*1卷积 5*5卷积 self.p3_1 = nn.Conv2d(in_channels=in_channels, out_channels=c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(in_channels=c3[0], out_channels=c3[1], kernel_size=5, padding=2) # 路线4 3*3最大池化 1*1卷积 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels=in_channels, out_channels=c4, kernel_size=1) def forward(self, x): p1 = self.ReLU(self.p1_1(x)) p2 = self.ReLU(self.p2_2(self.ReLU(self.p2_1(x)))) p3 = self.ReLU(self.p3_2(self.ReLU(self.p3_1(x)))) p4 = self.ReLU(self.p4_2(self.p4_1(x))) return torch.cat([p1,p2,p3,p4], 1) 3. GoogleNet模型搭建\rclass GoogLeNet(nn.Module): def __init__(self,Inception): super(GoogLeNet,self).__init__() self.relu = nn.ReLU() self.b1 = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b2 = nn.Sequential( nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b3 = nn.Sequential( Inception(192,64,[96,128],[16,32],32), Inception(256,128,[128,192],[32,96],64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b4 = nn.Sequential( Inception(480,192,[96,208],[16,48],64), Inception(512,160,[112,224],[24,64],64), Inception(512,128,[128,256],[24,64],64), Inception(512,112,[128,288],[32,64],64), Inception(528,256,[160,320],[32,128],128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b5 = nn.Sequential( Inception(832, 256, [160, 320], [32, 128], 128), Inception(832, 384, [192, 384], [48, 128], 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(1024,10), ) 4. 权重初始化\rfor m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=\u0026#39;fan_out\u0026#39;,nonlinearity=\u0026#39;relu\u0026#39;) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m,nn.Linear): nn.init.normal_(m.weight,0,0.01) nn.init.constant_(m.bias, 0) 5.模型前向传播\rdef forward(self, x): x = self.b1(x) x = self.b2(x) x = self.b3(x) x = self.b4(x) x = self.b5(x) return x 6.模型测试\rif __name__ == \u0026#39;__main__\u0026#39;: device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = GoogLeNet(Inception).to(device) print(summary(model,(1,224,224))) 完整代码展示\rimport torch from torch import nn from torchsummary import summary class Inception(nn.Module): def __init__(self, in_channels,c1,c2,c3,c4): super(Inception, self).__init__() self.ReLU = nn.ReLU() # 路线1 1*1卷积 self.p1_1 = nn.Conv2d(in_channels=in_channels, out_channels=c1, kernel_size=1) # 路线二 1*1卷积 3*3卷积 self.p2_1 = nn.Conv2d(in_channels=in_channels, out_channels=c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(in_channels=c2[0], out_channels=c2[1], kernel_size=3,padding=1) # 路线3 1*1卷积 5*5卷积 self.p3_1 = nn.Conv2d(in_channels=in_channels, out_channels=c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(in_channels=c3[0], out_channels=c3[1], kernel_size=5, padding=2) # 路线4 3*3最大池化 1*1卷积 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels=in_channels, out_channels=c4, kernel_size=1) def forward(self, x): p1 = self.ReLU(self.p1_1(x)) p2 = self.ReLU(self.p2_2(self.ReLU(self.p2_1(x)))) p3 = self.ReLU(self.p3_2(self.ReLU(self.p3_1(x)))) p4 = self.ReLU(self.p4_2(self.p4_1(x))) return torch.cat([p1,p2,p3,p4], 1) class GoogLeNet(nn.Module): def __init__(self,Inception): super(GoogLeNet,self).__init__() self.relu = nn.ReLU() self.b1 = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b2 = nn.Sequential( nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b3 = nn.Sequential( Inception(192,64,[96,128],[16,32],32), Inception(256,128,[128,192],[32,96],64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b4 = nn.Sequential( Inception(480,192,[96,208],[16,48],64), Inception(512,160,[112,224],[24,64],64), Inception(512,128,[128,256],[24,64],64), Inception(512,112,[128,288],[32,64],64), Inception(528,256,[160,320],[32,128],128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), ) self.b5 = nn.Sequential( Inception(832, 256, [160, 320], [32, 128], 128), Inception(832, 384, [192, 384], [48, 128], 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(1024,10), ) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode=\u0026#39;fan_out\u0026#39;,nonlinearity=\u0026#39;relu\u0026#39;) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m,nn.Linear): nn.init.normal_(m.weight,0,0.01) nn.init.constant_(m.bias, 0) def forward(self, x): x = self.b1(x) x = self.b2(x) x = self.b3(x) x = self.b4(x) x = self.b5(x) return x if __name__ == \u0026#39;__main__\u0026#39;: device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = GoogLeNet(Inception).to(device) print(summary(model,(1,224,224))) 总结\rGoogLeNet模型的搭建其中最难的是Inception模块的搭建，Inception模块的搭建需要注意的是每个路线的卷积层的输出通道数，以及最后拼接的通道数。\n","date": "2025-11-11 15:00:00",
    "updated": "2025-11-11 15:00:00"
  }, 
  {
    "objectID": "21aa5fcb60becf15fbe5dceba48cc7c060c961d4",
    "permalink": "/post/no.15/",
    "title": "GoogleNet模型原理","content": "\rGoogleNet诞生背景\rGoogleNet的模型的诞生主要是为了解决卷积核大小的问题，早期的模型使用的卷积核大小都是比较大的，比如11x11、5x5等， 而为了验证卷积核大小对模型的影响，GoogleNet创新型的使用了多个不同大小的卷积核，比如1x1、3x3、5x5等， 让模型自己去选择合适的卷积核大小，去更新相关的参数，从而提高了模型的表达能力。 而这个创新型的设计，也使得GoogleNet的模型在当时的时间点上，成为了一个比较有代表性的模型。 这个块提出者将他命名为Inception块，因为这个块的设计灵感来自于电影《盗梦空间》中的场景， 其中主角在梦境中被一个inception结构的神经网络所干扰，导致他的梦境被改变。\nInception块结构\rInception块详细参数\r1*1卷积核的优点\r全局平均池化优缺点\r全局平均池化的优点\r全局平均池化的缺点\rGoogleNet详细参数\r第1个inception块详细参数\r第2个inception块详细参数\r第3个inception块详细参数\r第4个inception块详细参数\r第5个inception块详细参数\r第6个inception块详细参数\r第7个inception块详细参数\r第8个inception块详细参数\r第9个inception块详细参数\r全连接层参数\r总结\r关于1*1卷积核的优点，全局平均池化的优缺点可以观看炮哥的视频：1*1卷积的优点 这些东西有点抽象，不太好用文字来叙述出来，这也是大模型的一个缺点吧，就是解释起来比较困难，不太好理解。所以看不懂图中的东西，可以看看去炮哥的视频，说不定就能理解了\n","date": "2025-11-11 15:00:00",
    "updated": "2025-11-11 15:00:00"
  }, 
  {
    "objectID": "8240cf29e1706aa336a5ba901acdca2481ef29a2",
    "permalink": "/post/no.14/",
    "title": "VGG模型-模型搭建","content": "\r前言\r关于VGG模型的训练代码和测试代码可以参考LeNet-5模型的训练代码和测试代码，只是模型不同。 修改一下其中的一些参数就行\n模型搭建\r1.导入必要的库\rimport torch from torch import nn from torchsummary import summary 2.VGG模型搭建\rclass VGG16(nn.Module): def __init__(self): super(VGG16, self).__init__() self.block1 = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block2 = nn.Sequential( nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block3 = nn.Sequential( nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block4 = nn.Sequential( nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block5 = nn.Sequential( nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block6 = nn.Sequential( nn.Flatten(), nn.Linear(7*7*512, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 10), ) 3.权重初始化\r# 权重初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, nonlinearity=\u0026#39;relu\u0026#39;) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) if m.bias is not None: nn.init.constant_(m.bias, 0) 4.模型的前向传播\rdef forward(self, x): x = self.block1(x) x = self.block2(x) x = self.block3(x) x = self.block4(x) x = self.block5(x) x = self.block6(x) return x 5.模型的测试\rif __name__==\u0026#34;__main__\u0026#34;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = VGG16().to(device) print(summary(model, (1, 224, 224))) 完整代码展示\rimport torch from torch import nn from torchsummary import summary class VGG16(nn.Module): def __init__(self): super(VGG16, self).__init__() self.block1 = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block2 = nn.Sequential( nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block3 = nn.Sequential( nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block4 = nn.Sequential( nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block5 = nn.Sequential( nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.block6 = nn.Sequential( nn.Flatten(), nn.Linear(7*7*512, 256), nn.ReLU(), nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 10), ) # 权重初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, nonlinearity=\u0026#39;relu\u0026#39;) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) if m.bias is not None: nn.init.constant_(m.bias, 0) def forward(self, x): x = self.block1(x) x = self.block2(x) x = self.block3(x) x = self.block4(x) x = self.block5(x) x = self.block6(x) return x if __name__==\u0026#34;__main__\u0026#34;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = VGG16().to(device) print(summary(model, (1, 224, 224))) 总结\rVGG模型的搭建与LeNet模型的搭建基本相同，只是VGG模型的卷积层更多，其余层的搭建与LeNet模型相同。 其中加入了一个权重初始化的函数，用于初始化模型的权重。 主要的目的是为了避免模型在训练过程中出现梯度消失或梯度爆炸的问题。 同时也可以提高模型的训练效率。 这里使用了Kaiming初始化方法，对卷积层的权重进行初始化。\n","date": "2025-11-10 15:00:00",
    "updated": "2025-11-10 15:00:00"
  }, 
  {
    "objectID": "8262b59feac9079cae55dab778b885eb44dcdfa2",
    "permalink": "/post/no.13/",
    "title": "VGG模型原理","content": "\rVGG诞生背景\rVGG的模型比AlexNet的更大，参数更多 在AlexNet的基础上，VGG提出了一种新的网络结构，即使用多个3x3的卷积核代替AlexNet中的11x11、5x5的卷积核，同时减少了参数数量。 由于3x3的卷积核可以覆盖更多的像素，因此VGG的模型在保持参数数量的同时，也提高了模型的表达能力。 卷积块的应用也使得VGG的模型可移植性比较好 本篇文章主要介绍VGG-16模型,也就是下面图中D的结构\nVGG-16模型结构\rVGG-16模型的结构与AlexNet的结构类似，都是由5个卷积块和3个全连接层组成。 每个卷积块由2个卷积层和1个池化层组成，卷积层的核大小都是3x3，步长都是1，填充都是1。 池化层的核大小都是2x2，步长都是2，填充都是0。 全连接层的节点数分别是4096、4096、1000。\n卷积块结构\rblock1详细参数\rblock2详细参数\rblock3详细参数\rblock4详细参数\rblock5详细参数\r总结\r","date": "2025-11-10 15:00:00",
    "updated": "2025-11-10 15:00:00"
  }, 
  {
    "objectID": "b06e69553386afbdaeae5fe774185c4c65b95658",
    "permalink": "/post/no.12/",
    "title": "AlexNet模型-模型代码搭建","content": "\r前言\r之后的文章只会介绍模型的代码搭建，而训练代码和测试代码 在链接里面有详细的代码，这里就不重复介绍了。\n模型代码搭建\r1. 导入所需的库\rimport torch from torch import nn from torchsummary import summary import torch.nn.functional as F 2. 模型搭建\rclass AlexNet(nn.Module): def __init__(self): super(AlexNet, self).__init__() self.ReLU = nn.ReLU() self.c1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4) self.s2 =nn.MaxPool2d(kernel_size=3, stride=2) self.c3 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1,padding=2) self.s4 = nn.MaxPool2d(kernel_size=3, stride=2) self.c5 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1) self.c6 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1) self.c7 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1) self.s8 = nn.MaxPool2d(kernel_size=3, stride=2) self.flatten = nn.Flatten() self.fc1 = nn.Linear(in_features=6*6*256, out_features=4096) self.fc2 = nn.Linear(in_features=4096, out_features=4096) self.fc3 = nn.Linear(in_features=4096, out_features=10) 其中c代表卷积层，s代表池化层，fc代表全连接层。\n3. 模型前向传播\rdef forward(self, x): x = self.ReLU(self.c1(x)) x = self.s2(x) x = self.ReLU(self.c3(x)) x = self.s4(x) x = self.ReLU(self.c5(x)) x = self.ReLU(self.c6(x)) x = self.ReLU(self.c7(x)) x = self.s8(x) x = self.flatten(x) x = self.ReLU(self.fc1(x)) x = F.dropout(x, p=0.5) x = self.ReLU(self.fc2(x)) x = F.dropout(x, p=0.5) x = self.fc3(x) return x 4. 模型测试代码\rif __name__ == \u0026#34;__main__\u0026#34;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = AlexNet().to(device) print(summary(model, (1, 227, 227))) 完整模型代码展示\rimport torch from torch import nn from torchsummary import summary import torch.nn.functional as F class AlexNet(nn.Module): def __init__(self): super(AlexNet, self).__init__() self.ReLU = nn.ReLU() self.c1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4) self.s2 =nn.MaxPool2d(kernel_size=3, stride=2) self.c3 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1,padding=2) self.s4 = nn.MaxPool2d(kernel_size=3, stride=2) self.c5 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1) self.c6 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1) self.c7 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1) self.s8 = nn.MaxPool2d(kernel_size=3, stride=2) self.flatten = nn.Flatten() self.fc1 = nn.Linear(in_features=6*6*256, out_features=4096) self.fc2 = nn.Linear(in_features=4096, out_features=4096) self.fc3 = nn.Linear(in_features=4096, out_features=10) def forward(self, x): x = self.ReLU(self.c1(x)) x = self.s2(x) x = self.ReLU(self.c3(x)) x = self.s4(x) x = self.ReLU(self.c5(x)) x = self.ReLU(self.c6(x)) x = self.ReLU(self.c7(x)) x = self.s8(x) x = self.flatten(x) x = self.ReLU(self.fc1(x)) x = F.dropout(x, p=0.5) x = self.ReLU(self.fc2(x)) x = F.dropout(x, p=0.5) x = self.fc3(x) return x if __name__ == \u0026#34;__main__\u0026#34;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = AlexNet().to(device) print(summary(model, (1, 227, 227))) ","date": "2025-11-09 15:00:00",
    "updated": "2025-11-09 15:00:00"
  }, 
  {
    "objectID": "5b1be64b913b9031a6b524df9344c8155b693d64",
    "permalink": "/post/no.10/",
    "title": "LeNet模型-测试代码搭建","content": "\r前言\r同样这一套的代码依旧是一套通用的代码，可以反复于其他模型的测试。\n测试代码搭建\r1. 导入必要的库\rimport torch import torch.utils.data as Data from torchvision import transforms from torchvision.datasets import FashionMNIST from model import LeNet 同样的，这里的模型文件也需要和测试代码在同一个目录下，并且要和模型文件的文件名一致。 可以参考\nfrom 你模型的文件名 import LeNet 2.数据处理\rdef test_data_process(): test_data = FashionMNIST(root=\u0026amp;#39;./data\u0026amp;#39;, train=False, transform=transforms.Compose([transforms.Resize(size=227), transforms.ToTensor()]), download=True) test_dataloader = Data.DataLoader(dataset=test_data, batch_size=1, shuffle=True, num_workers=0) return test_dataloader 3.模型测试代码\rdef test_model_process(model, test_dataloader): # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU device = \u0026amp;#34;cuda\u0026amp;#34; if torch.cuda.is_available() else \u0026amp;#39;cpu\u0026amp;#39; # 讲模型放入到训练设备中 model = model.to(device) # 初始化参数 test_corrects = 0.0 test_num = 0 # 只进行前向传播计算，不计算梯度，从而节省内存，加快运行速度 with torch.no_grad(): for test_data_x, test_data_y in test_dataloader: # 将特征放入到测试设备中 test_data_x = test_data_x.to(device) # …","date": "2025-11-08 15:00:00",
    "updated": "2025-11-08 15:00:00"
  }, 
  {
    "objectID": "09bd8c18769cb0676c19e5e7589342a0c2bc6ce5",
    "permalink": "/post/no.11/",
    "title": "LeNet模型-模型代码搭建","content": "\r前言\r这篇文章主要为LeNet模型的代码搭建，代码主要参考了炮哥带你学的视频，但是我自己也添加了一些注释，希望可以帮助到大家。 代码运行的详细环境搭建可以参考炮哥的视频代码环境搭建\n模型代码搭建\r1. 导入必要的库\rimport torch from torch import nn from torchsummary import summary import torch.nn.functional as F 2. 搭建LeNet模型\rclass LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5,padding=2) self.sig = nn.Sigmoid() self.s2 = nn.AvgPool2d(kernel_size=2, stride=2) self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.s4 = nn.AvgPool2d(kernel_size=2, stride=2) self.flatten = nn.Flatten() self.f5 = nn.Linear(in_features=16*5*5, out_features=120) self.f6 = nn.Linear(in_features=120, out_features=84) self.f7 = nn.Linear(in_features=84, out_features=10) 其中c代表卷积层，s代表池化层，fc代表全连接层。\n3. 模型前向传播\rdef forward(self, x): x = self.sig(self.c1(x)) x = self.s2(x) x = self.sig(self.c3(x)) x = self.s4(x) x = self.flatten(x) x = self.f5(x) x = self.f6(x) x = self.f7(x) return x 4. 模型测试代码\rif __name__ == \u0026#39;__main__\u0026#39;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = LeNet().to(device) print(summary(model, (1, 28, 28))) 完整模型代码\rimport torch from torch import nn from torchsummary import summary class LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5,padding=2) self.sig = nn.Sigmoid() self.s2 = nn.AvgPool2d(kernel_size=2, stride=2) self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.s4 = nn.AvgPool2d(kernel_size=2, stride=2) self.flatten = nn.Flatten() self.f5 = nn.Linear(in_features=16*5*5, out_features=120) self.f6 = nn.Linear(in_features=120, out_features=84) self.f7 = nn.Linear(in_features=84, out_features=10) def forward(self, x): x = self.sig(self.c1(x)) x = self.s2(x) x = self.sig(self.c3(x)) x = self.s4(x) x = self.flatten(x) x = self.f5(x) x = self.f6(x) x = self.f7(x) return x if __name__ == \u0026#39;__main__\u0026#39;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = LeNet().to(device) print(summary(model, (1, 28, 28))) ","date": "2025-11-08 15:00:00",
    "updated": "2025-11-08 15:00:00"
  }, 
  {
    "objectID": "e40b2dcacce9614e71e176b88c4ce4ec40e43809",
    "permalink": "/post/no.9/",
    "title": "LeNet模型-训练代码搭建","content": "\r前言\r本片文章主要是通用的模型训练代码搭建，这个代码搭建完成后，我们就可以直接使用这个代码来训练我们的模型了。 而且后面的模型都可以复用这一套的代码，只需要修改一下模型的参数即可。\n代码搭建\r1.导入必要的库\r在这一步中，我们需要导入一些必要的库，这些库包括：\ncopy：用于复制对象 time：用于计算时间 torch：用于深度学习 torchvision：用于处理图像数据 numpy：用于数值计算 matplotlib：用于数据可视化 modle：用于导入模型 torch.nn：用于定义神经网络层 pandas：用于数据处理 import copy import time import torch from torchvision.datasets import FashionMNIST from torchvision import transforms import torch.utils.data as Data import numpy as np import matplotlib.pyplot as plt from model import LeNet import torch.nn as nn import pandas as pd 有关库的导入这里其中有一个库是model，这个是我们搭建模型时自己创建的一个文件，所以在引入的时候要和模型文件在同一个目录下，并且要和模型文件的文件名一致。 就像时我的模型文件是model.py，所以在引入的时候要写\nfrom model import LeNet 而你们的就是\nfrom 你模型的文件名 import LeNet 2. 数据处理\rdef train_val_data_process(): # 从FashionMNIST中导入训练数据，将数据转换为227*227的大小，同时将数据转换为tensor类型 train_data = FashionMNIST(root=\u0026amp;#39;./data\u0026amp;#39;, train=True, transform=transforms.Compose([transforms.Resize(size=227), transforms.ToTensor()]), download=True) # 划分训练集和验证集，训练集占80%，验证集占20% train_data, …","date": "2025-11-07 15:00:00",
    "updated": "2025-11-07 15:00:00"
  }, 
  {
    "objectID": "6e41b93accb5d6faf5456a18f36f45c2689f1da9",
    "permalink": "/post/no.8/",
    "title": "AlexNet原理","content": "\r1. 背景\rAlexNet 是 2012 年由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 提出的卷积神经网络架构。它在 ImageNet 图像分类任务上取得了显著的成果，成为了卷积神经网络的一个重要里程碑。 它在当年的 ImageNet 图像分类任务上，取得第一名的成绩，也向世界证明了卷积神经网络在图像识别任务上要优于传统的机器学习算法。\n2. AlexNet 网络结构\r3. 网络参数详解\r4. Dropout 层\r为什么要加入 Dropout 层？\n其实是为了降低网络的参数，由于层数的增加，网络的参数也会增加，而 Dropout 层可以随机的将一些神经元的输出设为 0，从而减少网络的参数。 加快了网络的训练速度，同时也提高了网络的泛化能力。\n5. 图像增强\r5.1 水平翻转\r将图像左右翻转，增加数据集的多样性。\n5.2 随机裁剪\r从图像中随机裁剪出一个子区域，增加数据集的多样性。\n5.3 PCA\rPCA：主成分分析，将图像的像素值转换为主成分，从而减少图像的维度。\n5.4 LRN正则化\rLRN 正则化：局部响应归一化，对每个像素的响应进行归一化，从而提高模型的泛化能力。\n总的来说，图像增强技术可以增加数据集的多样性，提高模型的泛化能力。\n6. 总结\r","date": "2025-11-06 15:00:00",
    "updated": "2025-11-06 15:00:00"
  }, 
  {
    "objectID": "d5bdae485b1876705d0acd377b12e3d7d9cdd7e6",
    "permalink": "/post/no.7/",
    "title": "LeNet原理","content": "\r1. LeNet-5 诞生背景\rLenet-5 是 Yann LeCun 于 1998 年提出的卷积神经网络（Convolutional Neural Network，CNN）架构，用于识别手写数字。\n2. LeNet-5 网络结构\rLeNet-5 由 7 层组成，包括 2 个卷积层、2 个池化层、3 个全连接层。\nLeNet-5 后面的尾标5其实指的是它有2个卷积层，3个全连接层。池化层并不算在其中，包括后面的模型也是如此，尾标只包括卷积层和全连接层的数量。\n3. LeNet-5 网络层详解\r3.1 LeNet-5 网络参数详解\r4. 总结\r","date": "2025-11-06 14:00:00",
    "updated": "2025-11-06 14:00:00"
  }, 
  {
    "objectID": "60b6657f0eb1a28c32117a58cebecdae75db68c6",
    "permalink": "/post/no.6/",
    "title": "CNN卷积神经网络算法原理：第三章","content": "\r图片在计算机中的本质\r单色图片\r彩色图片\r其实从图片中我们可以看出，图片在计算机中的本质就是一个二维数组，每个元素就是一个像素点，每个像素点有三个通道，分别是RGB通道，每个通道的值范围是0-255，0表示该通道的亮度最低，255表示该通道的亮度最高。\n全连接神经网络在图像识别中的问题\r从全连接神经网络的结构我们不难看出，图片是一个二维数组，而全连接神经网络的输入层是一个一维数组，所以我们需要将图片展开成一个一维数组，才能输入到全连接神经网络中。 但是，将图片展开成一个一维数组会导致一个问题，就是图片的空间信息会丢失。\n例如，一张图片是一个28x28的灰度图片，展开成一个一维数组就是784个元素，每个元素就是一个像素点的灰度值。 但是，我们可以看出，图片中存在着空间信息，例如，一个像素点的灰度值和它的邻居像素点的灰度值是相关的。\n如果我们将图片展开成一个一维数组，那么这些空间信息就会丢失，模型就无法利用这些空间信息来进行识别。 为了解决上面的问题，我们需要引入卷积这个操作来提取图片的空间信息，以保留图片的空间结构特征。\n卷积层\r卷积运算\r不加偏置项的卷积运算\r卷积运算的过程就是将卷积核在图片上滑动，每次滑动一个像素，计算卷积核和图片上对应位置的元素的乘积，然后将这些乘积相加，得到卷积结果。 总结就一句话，先相乘，在相加\n加入偏置项的卷积运算\r加入偏置项的卷积运算就是在卷积运算的基础上，加上一个偏置项b，最终的卷积结果就是卷积核和图片上对应位置的元素的乘积的和再加上偏置项b。\n填充\r填充就是在图片的边界上添加一些像素点，以保持卷积后的图片大小不变。\n步幅\r步幅就是卷积核在图片上滑动的步长，步幅越大，卷积后的图片越小。\n卷积公式\r卷积公式就是根据卷积核的大小、填充、步幅等参数，计算出卷积后的图片大小。\n多通道卷积\r多通道卷积就是在每个通道上进行卷积运算，最后将所有通道的卷积结果合并起来。\n利用立体图来表示卷积\r池化\r最大池化\r最大池化就是在每个池化窗口中，取窗口内的最大值作为池化结果。\n平均池化\r平均池化就是在每个池化窗口中，取窗口内的平均值作为池化结果。\n其实池化后的特征图大小的计算公式跟卷积是一样的，是通用的而且及其重要。\n卷积神经网络的整体结构\r总结\r卷积神经网络其实就是在全连接神经网络的基础上迭代而来的，它解决了全连接神经网络在图像识别中的问题，即图片的空间信 …","date": "2025-11-05 15:00:00",
    "updated": "2025-11-05 15:00:00"
  }, 
  {
    "objectID": "f39e7c3061801b6c2e3b38f34df2fecb6b257190",
    "permalink": "/post/no.5/",
    "title": "CNN卷积神经网络算法原理：第二章","content": "\r前向传播\r如下图所示，这就是神经网络前向传播的计算过程\n而下图所示，是一个具体的前向传播计算过程\n损失函数\r在神经网络中我们一般使用的是均方误差损失函数，公式如下：\n反向传播\r如下图所示，这就是神经网络反向传播的一个案例\n通常反向传播的计算过程如下： 1. 求参数的梯度 2. 利用参数的梯度进行更新 3. 新参数的前向传播\n下面我们就根据一个简单的案例来具体演示一下反向传播的过程。\n求参数的梯度\r利用参数的梯度进行更新\r新参数的前向传播\r总结\r神经网络反向传播的过程就是根据损失函数对参数的梯度，利用梯度下降法对参数进行更新，运用新的参数进行前向传播，从而不断的去优化模型，直到模型的损失函数收敛到一个最优解。\n","date": "2025-11-04 15:00:00",
    "updated": "2025-11-04 15:00:00"
  }, 
  {
    "objectID": "58cdda505b18fd1ab2ebbf7e0c0fe01430bbdaa5",
    "permalink": "/post/no.4/",
    "title": "CNN卷积神经网络算法原理：第一章","content": "\r全连接神经网络整体结构\r全连接神经网络的整体结构如下：\n上述图片就是一个全连接神经网络的整体结构，它基本上有以下几个部分：\n输入层：输入层主要是接收外部输入的数据 隐藏层：隐藏层可以有多个，每个隐藏层由多个神经元组成，每个神经元接收来自上一层的所有节点的输入 输出层：输出层的节点数等于分类的类别数，每个节点对应一个类别。 全连接神经网络的单元结构\r每个神经元的计算过程如下：\n由图中不难看出，它的计算过程神似我们大脑中的神经元结构，这也就是为啥要叫全连接神经网络。 全连接神经网络与卷积神经网络的区别就是其中的神经元，其中CNN卷积神经网络的神经元其实就是一个个卷积核，将全连接神经网络中的每个神经元都替换为卷积核，就得到了卷积神经网络。\n激活函数\r激活函数的作用就是将神经元的输出映射到一个指定的范围，常用的激活函数有sigmoid函数、tanh函数、ReLU函数等。\n为啥要加入激活函数呢？ 其一因为如果不加入激活函数，那么神经网络就变成了一个线性模型，无法解决非线性问题。 其二就是加入激活函数可以使神经网络的输出在一个指定的范围内，这对于分类问题尤为重要。 其三就是加入激活函数可以使神经网络的训练更加高效，因为激活函数的导数在大部分情况下都是一个常数，这就使得反向传播算法的计算更加简单。\nsigmoid函数\rsigmoid函数的图像如下：\ntanh函数\rtanh函数的图像如下：\nReLU函数\rReLU函数的图像如下：\nLeaky ReLU函数\rLeaky ReLU函数的图像如下：\n总结\r这一章简单介绍了一下全连接神经网络的整体结构、单元结构、激活函数，以及它们的作用。\n","date": "2025-11-03 15:00:00",
    "updated": "2025-11-03 15:00:00"
  }, 
  {
    "objectID": "95515b1ac18353ab95200c470beac299822046d0",
    "permalink": "/post/no.3/",
    "title": "线性回归模型案例的代码复现","content": "\r案例\r上述就是我们要解决的问题，即通过线性回归模型来对未知的y值进行预测。下面我们将通过代码复现一下这个案例。\n代码复现\r# 定义数据集 # 定义数据特征 x_data = [1, 2, 3] # 定义数据标签 y_data = [2, 4, 6] # 初始化参数W w = 4 #定义线性回归模型 def forword(x): return x * w #定义损失函数 def cost(xs, ys): costvalue = 0 for x , y in zip(xs, ys): y_pred = forword(x) costvalue += (y_pred - y)**2 return costvalue / len(xs) #定义计算梯度的函数 def gradient(xs, ys): grad = 0 for x, y in zip(xs, ys): grad += 2 * x * (x * w - y) return grad / len(xs) for epoch in range(100): #计算误差损失 cost_val = cost(x_data, y_data) grad_val = gradient(x_data, y_data) w =w-0.01 * grad_val print(\u0026#39;训练轮次：\u0026#39;, epoch, \u0026#34;w=\u0026#34;, w,\u0026#34;loss\u0026#34;, cost_val) 结果\r运行上述的代码，我们不难得出当w接近2时，损失函数也接近0，这也验证了我们的模型是正确的。\n","date": "2025-11-02 15:00:00",
    "updated": "2025-11-02 15:00:00"
  }, 
  {
    "objectID": "4981202e06eb49e7473b6db82030b5bac0fff769",
    "permalink": "/post/no.2/",
    "title": "梯度更新","content": "\r梯度下降法\r根据上一篇的文章我们知道，我们可以用最小二乘法根据损失函数与参数w来求w的最优解。具体该怎么做呢？我们可以联想到以下的场景：\n我们根据以上场景，再结合损失函数与参数w的关系，我们可以得到下图中的公式\n该公式就是梯度下降法的公式，但其中具体是怎么来到这就可以参考高中数学的微积分中的微分原理。\n上图就是一个简单的微分原理演示图，其中我们不难看出梯度下降法中的a可以类比为微分中的∆Xm，而我们该如何求出这一段X值所对应的y值呢，其实很简单，我们拿最近简单一阶方程y=kx举例，我们对X进行求导，就可以得到k的值，而对应的y值就是k*Xm。 而在梯度下降法中，在我们给定一个初始值w0后，我们就可以根据梯度下降法的公式，不断的去更新w0，直到w0收敛到一个最优解。也就是图像中的最低点所以在梯度下降法中，学习率a对模型的影响极大，它不仅影响了模型的收敛速度，还影响了模型的精度；当我们设置a时要慎重考虑。\n案例求解\r下面我们根据以上梯度下降法的求解过程，对上一篇文章中的案例进行求解 参考以下的案例，我们不难求解出上一篇文章中的案例，以下的图片就是求解过程 总结\r根据上述论断我们不难看出梯度下降法实际上就是根据微分的思想，将我们寻找损失函数的最小值的过程，在a的控制下，进行不断的细化拆分，直到我们找到一个最优解。\n","date": "2025-11-01 15:00:00",
    "updated": "2025-11-01 15:00:00"
  }, 
  {
    "objectID": "9bc61343927d1df4cc49649fdcf243c9cb58447f",
    "permalink": "/post/no.1/",
    "title": "线性回归原理详解","content": "\r线性回归模型\r参考人类的学习方式，我们一般学会识别一个物体，都是通过观察大量的样本，然后根据样本中的特征进行识别的；同理机器学习也是如此，其中我们将事物的名称命名为标签及为y或，而特征则为x。机器学习就是通过学习大量的样本，然后根据样本中的标签去获得样品的特征，从而对新的样本进行预测。 就像我们如何判断一个人是否为男性，我们一般会根据他的特征来判断，比如他是否有长的头发、是否有喉结等；而机器学习就是通过学习大量的样本，然后根据样本中的特征去判断新的样本是否为男性。 所以线性回归模型就可以简化为y关于x的求解，就是已知x，然后根据x去预测y。而机器学习的目的就是确定x的系数，从而对新的样本进行预测。\n案例\r如同以上案例，我们希望通过已有的学习时间，去预测未知学习时间的考试分数。\n误差函数\r参考上述的案例，我们根据学习时间和考试分数的落点分布，设线性回归函数，同时为了避免我们获得的结果与现实的结果出现很大的偏差，我们需要引入误差函数。误差函数的作用就是衡量我们获得的结果与现实的结果之间的差异，从而判断我们的解是否准确。下面我们就对该回归函数求解\n穷举法\r穷举法是一种简单的求解方法，它的基本思想是：在所有可能的解中，选择一个最优解。但当我们的解空间很大时，穷举法的效率就会很低。所以我们引入了其他的求解方法，那个方法就是最小二乘法\n最小二乘法\r我们根据穷举法可以画出误差函数的图像，而最小二乘法的基本思想就是：在所有可能的解中，选择一个使得误差函数最小的解。以这个解为基础，我们可以得到线性回归函数的系数。\n求解\r下面我们根据以上最小二乘法的求解过程，对线性回归模型进行求解\n","date": "2025-10-30 15:00:00",
    "updated": "2025-10-30 15:00:00"
  }]