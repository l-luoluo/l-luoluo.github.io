[
  {
    "objectID": "f39e7c3061801b6c2e3b38f34df2fecb6b257190",
    "permalink": "/post/no.5/",
    "title": "CNN卷积神经网络算法原理 第二章","content": "\r前向传播\r如下图所示，这就是神经网络前向传播的计算过程\n而下图所示，是一个具体的前向传播计算过程\n损失函数\r在神经网络中我们一般使用的是均方误差损失函数，公式如下：\n反向传播\r如下图所示，这就是神经网络反向传播的一个案例\n通常反向传播的计算过程如下： 1. 求参数的梯度 2. 利用参数的梯度进行更新 3. 新参数的前向传播\n下面我们就根据一个简单的案例来具体演示一下反向传播的过程。\n求参数的梯度\r利用参数的梯度进行更新\r新参数的前向传播\r总结\r神经网络反向传播的过程就是根据损失函数对参数的梯度，利用梯度下降法对参数进行更新，运用新的参数进行前向传播，从而不断的去优化模型，直到模型的损失函数收敛到一个最优解。\n","date": "2025-11-04 15:00:00",
    "updated": "2025-11-04 15:00:00"
  }, 
  {
    "objectID": "58cdda505b18fd1ab2ebbf7e0c0fe01430bbdaa5",
    "permalink": "/post/no.4/",
    "title": "CNN卷积神经网络算法原理 第一章","content": "\r全连接神经网络整体结构\r全连接神经网络的整体结构如下：\n上述图片就是一个全连接神经网络的整体结构，它基本上有以下几个部分：\n输入层：输入层主要是接收外部输入的数据 隐藏层：隐藏层可以有多个，每个隐藏层由多个神经元组成，每个神经元接收来自上一层的所有节点的输入 输出层：输出层的节点数等于分类的类别数，每个节点对应一个类别。 全连接神经网络的单元结构\r每个神经元的计算过程如下：\n由图中不难看出，它的计算过程神似我们大脑中的神经元结构，这也就是为啥要叫全连接神经网络。 全连接神经网络与卷积神经网络的区别就是其中的神经元，其中CNN卷积神经网络的神经元其实就是一个个卷积核，将全连接神经网络中的每个神经元都替换为卷积核，就得到了卷积神经网络。\n激活函数\r激活函数的作用就是将神经元的输出映射到一个指定的范围，常用的激活函数有sigmoid函数、tanh函数、ReLU函数等。\n为啥要加入激活函数呢？ 其一因为如果不加入激活函数，那么神经网络就变成了一个线性模型，无法解决非线性问题。 其二就是加入激活函数可以使神经网络的输出在一个指定的范围内，这对于分类问题尤为重要。 其三就是加入激活函数可以使神经网络的训练更加高效，因为激活函数的导数在大部分情况下都是一个常数，这就使得反向传播算法的计算更加简单。\nsigmoid函数\rsigmoid函数的定义如下： $$\rsigmoid(x) = \\frac{1}{1 + e^{-x}}\r$$sigmoid函数的图像如下：\ntanh函数\rtanh函数的定义如下： $$\rtanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\r$$tanh函数的图像如下：\nReLU函数\rReLU函数的定义如下： $$\rReLU(x) = max(0, x)\r$$ReLU函数的图像如下：\nLeaky ReLU函数\rLeaky ReLU函数的定义如下： $$\rLeakyReLU(x) = max(0.01x, x)\r$$Leaky ReLU函数的图像如下：\n总结\r这一章简单介绍了一下全连接神经网络的整体结构、单元结构、激活函数，以及它们的作用。\n","date": "2025-11-03 15:00:00",
    "updated": "2025-11-03 15:00:00"
  }, 
  {
    "objectID": "578f610c073ea71d4b657164e43668353f84d344",
    "permalink": "/post/no3/",
    "title": "线性回归模型案例的代码复现","content": "\r案例\r上述就是我们要解决的问题，即通过线性回归模型来对未知的y值进行预测。下面我们将通过代码复现一下这个案例。\n代码复现\r# 定义数据集 # 定义数据特征 x_data = [1, 2, 3] # 定义数据标签 y_data = [2, 4, 6] # 初始化参数W w = 4 #定义线性回归模型 def forword(x): return x * w #定义损失函数 def cost(xs, ys): costvalue = 0 for x , y in zip(xs, ys): y_pred = forword(x) costvalue += (y_pred - y)**2 return costvalue / len(xs) #定义计算梯度的函数 def gradient(xs, ys): grad = 0 for x, y in zip(xs, ys): grad += 2 * x * (x * w - y) return grad / len(xs) for epoch in range(100): #计算误差损失 cost_val = cost(x_data, y_data) grad_val = gradient(x_data, y_data) w =w-0.01 * grad_val print(\u0026#39;训练轮次：\u0026#39;, epoch, \u0026#34;w=\u0026#34;, w,\u0026#34;loss\u0026#34;, cost_val) 结果\r运行上述的代码，我们不难得出当w接近2时，损失函数也接近0，这也验证了我们的模型是正确的。\n","date": "2025-11-02 15:00:00",
    "updated": "2025-11-02 15:00:00"
  }, 
  {
    "objectID": "fdad2c11486491c4f54f7c4d1c9c423d8c4e5f0a",
    "permalink": "/post/no2/",
    "title": "梯度更新","content": "\r梯度下降法\r根据上一篇的文章我们知道，我们可以用最小二乘法根据损失函数与参数w来求w的最优解。具体该怎么做呢？我们可以联想到以下的场景：\n我们根据以上场景，再结合损失函数与参数w的关系，我们可以得到下图中的公式\n该公式就是梯度下降法的公式，但其中具体是怎么来到这就可以参考高中数学的微积分中的微分原理。\n上图就是一个简单的微分原理演示图，其中我们不难看出梯度下降法中的a可以类比为微分中的∆Xm，而我们该如何求出这一段X值所对应的y值呢，其实很简单，我们拿最近简单一阶方程y=kx举例，我们对X进行求导，就可以得到k的值，而对应的y值就是k*Xm。 而在梯度下降法中，在我们给定一个初始值w0后，我们就可以根据梯度下降法的公式，不断的去更新w0，直到w0收敛到一个最优解。也就是图像中的最低点所以在梯度下降法中，学习率a对模型的影响极大，它不仅影响了模型的收敛速度，还影响了模型的精度；当我们设置a时要慎重考虑。\n案例求解\r下面我们根据以上梯度下降法的求解过程，对上一篇文章中的案例进行求解 参考以下的案例，我们不难求解出上一篇文章中的案例，以下的图片就是求解过程 总结\r根据上述论断我们不难看出梯度下降法实际上就是根据微分的思想，将我们寻找损失函数的最小值的过程，在a的控制下，进行不断的细化拆分，直到我们找到一个最优解。\n","date": "2025-11-01 15:00:00",
    "updated": "2025-11-01 15:00:00"
  }, 
  {
    "objectID": "9bc61343927d1df4cc49649fdcf243c9cb58447f",
    "permalink": "/post/no.1/",
    "title": "线性回归原理详解","content": "\r线性回归模型\r参考人类的学习方式，我们一般学会识别一个物体，都是通过观察大量的样本，然后根据样本中的特征进行识别的；同理机器学习也是如此，其中我们将事物的名称命名为标签及为y或，而特征则为x。机器学习就是通过学习大量的样本，然后根据样本中的标签去获得样品的特征，从而对新的样本进行预测。 就像我们如何判断一个人是否为男性，我们一般会根据他的特征来判断，比如他是否有长的头发、是否有喉结等；而机器学习就是通过学习大量的样本，然后根据样本中的特征去判断新的样本是否为男性。 所以线性回归模型就可以简化为y关于x的求解，就是已知x，然后根据x去预测y。而机器学习的目的就是确定x的系数，从而对新的样本进行预测。\n案例\r如同以上案例，我们希望通过已有的学习时间，去预测未知学习时间的考试分数。\n误差函数\r参考上述的案例，我们根据学习时间和考试分数的落点分布，设线性回归函数，同时为了避免我们获得的结果与现实的结果出现很大的偏差，我们需要引入误差函数。误差函数的作用就是衡量我们获得的结果与现实的结果之间的差异，从而判断我们的解是否准确。下面我们就对该回归函数求解\n穷举法\r穷举法是一种简单的求解方法，它的基本思想是：在所有可能的解中，选择一个最优解。但当我们的解空间很大时，穷举法的效率就会很低。所以我们引入了其他的求解方法，那个方法就是最小二乘法\n最小二乘法\r我们根据穷举法可以画出误差函数的图像，而最小二乘法的基本思想就是：在所有可能的解中，选择一个使得误差函数最小的解。以这个解为基础，我们可以得到线性回归函数的系数。\n求解\r下面我们根据以上最小二乘法的求解过程，对线性回归模型进行求解\n","date": "2025-10-30 15:00:00",
    "updated": "2025-10-30 15:00:00"
  }]