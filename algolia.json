[
  {
    "objectID": "5b1be64b913b9031a6b524df9344c8155b693d64",
    "permalink": "/post/no.10/",
    "title": "LeNet模型-测试代码搭建","content": "\r前言\r同样这一套的代码依旧是一套通用的代码，可以反复于其他模型的测试。\n测试代码搭建\r1. 导入必要的库\rimport torch import torch.utils.data as Data from torchvision import transforms from torchvision.datasets import FashionMNIST from modle import AlexNet 2.数据处理\rdef test_data_process(): test_data = FashionMNIST(root=\u0026amp;#39;./data\u0026amp;#39;, train=False, transform=transforms.Compose([transforms.Resize(size=227), transforms.ToTensor()]), download=True) test_dataloader = Data.DataLoader(dataset=test_data, batch_size=1, shuffle=True, num_workers=0) return test_dataloader 3.模型测试代码\rdef test_model_process(model, test_dataloader): # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU device = \u0026amp;#34;cuda\u0026amp;#34; if torch.cuda.is_available() else \u0026amp;#39;cpu\u0026amp;#39; # 讲模型放入到训练设备中 model = model.to(device) # 初始化参数 test_corrects = 0.0 test_num = 0 # 只进行前向传播计算，不计算梯度，从而节省内存，加快运行速度 with torch.no_grad(): for test_data_x, test_data_y in test_dataloader: # 将特征放入到测试设备中 test_data_x = test_data_x.to(device) # 将标签放入到测试设备中 test_data_y = test_data_y.to(device) # 设置模型为评估模式 model.eval() # 前向传播过程，输入为测 …","date": "2025-11-08 15:00:00",
    "updated": "2025-11-08 15:00:00"
  }, 
  {
    "objectID": "09bd8c18769cb0676c19e5e7589342a0c2bc6ce5",
    "permalink": "/post/no.11/",
    "title": "LeNet模型-模型代码搭建","content": "\r前言\r这篇文章主要为LeNet模型的代码搭建，代码主要参考了炮哥带你学的视频，但是我自己也添加了一些注释，希望可以帮助到大家。 代码运行的详细环境搭建可以参考炮哥的视频代码环境搭建\n模型代码搭建\r1. 导入必要的库\rimport torch from torch import nn from torchsummary import summary import torch.nn.functional as F 2. 搭建LeNet模型\rclass AlexNet(nn.Module): def __init__(self): super(AlexNet, self).__init__() self.ReLU = nn.ReLU() self.c1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4) self.s2 =nn.MaxPool2d(kernel_size=3, stride=2) self.c3 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1,padding=2) self.s4 = nn.MaxPool2d(kernel_size=3, stride=2) self.c5 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1) self.c6 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1) self.c7 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1) self.s8 = nn.MaxPool2d(kernel_size=3, stride=2) self.flatten = nn.Flatten() self.fc1 = nn.Linear(in_features=6*6*256, out_features=4096) self.fc2 = nn.Linear(in_features=4096, out_features=4096) self.fc3 = nn.Linear(in_features=4096, out_features=10) 3. 模型前向传播\rdef forward(self, x): x = self.ReLU(self.c1(x)) x = self.s2(x) x = self.ReLU(self.c3(x)) x = self.s4(x) x = self.ReLU(self.c5(x)) x = self.ReLU(self.c6(x)) x = self.ReLU(self.c7(x)) x = self.s8(x) x = self.flatten(x) x = self.ReLU(self.fc1(x)) x = F.dropout(x, p=0.5) x = self.ReLU(self.fc2(x)) x = F.dropout(x, p=0.5) x = self.fc3(x) return x 4. 模型测试\rif __name__ == \u0026#34;__main__\u0026#34;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = AlexNet().to(device) print(summary(model, (1, 227, 227))) 完整模型代码\rimport torch from torch import nn from torchsummary import summary import torch.nn.functional as F class AlexNet(nn.Module): def __init__(self): super(AlexNet, self).__init__() self.ReLU = nn.ReLU() self.c1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4) self.s2 =nn.MaxPool2d(kernel_size=3, stride=2) self.c3 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1,padding=2) self.s4 = nn.MaxPool2d(kernel_size=3, stride=2) self.c5 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1) self.c6 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1) self.c7 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1) self.s8 = nn.MaxPool2d(kernel_size=3, stride=2) self.flatten = nn.Flatten() self.fc1 = nn.Linear(in_features=6*6*256, out_features=4096) self.fc2 = nn.Linear(in_features=4096, out_features=4096) self.fc3 = nn.Linear(in_features=4096, out_features=10) def forward(self, x): x = self.ReLU(self.c1(x)) x = self.s2(x) x = self.ReLU(self.c3(x)) x = self.s4(x) x = self.ReLU(self.c5(x)) x = self.ReLU(self.c6(x)) x = self.ReLU(self.c7(x)) x = self.s8(x) x = self.flatten(x) x = self.ReLU(self.fc1(x)) x = F.dropout(x, p=0.5) x = self.ReLU(self.fc2(x)) x = F.dropout(x, p=0.5) x = self.fc3(x) return x if __name__ == \u0026#34;__main__\u0026#34;: device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = AlexNet().to(device) print(summary(model, (1, 227, 227))) ","date": "2025-11-08 15:00:00",
    "updated": "2025-11-08 15:00:00"
  }, 
  {
    "objectID": "e40b2dcacce9614e71e176b88c4ce4ec40e43809",
    "permalink": "/post/no.9/",
    "title": "LeNet模型-训练代码搭建","content": "\r前言\r本片文章主要是通用的模型训练代码搭建，这个代码搭建完成后，我们就可以直接使用这个代码来训练我们的模型了。 而且后面的模型都可以复用这一套的代码，只需要修改一下模型的参数即可。\n代码搭建\r1.导入必要的库\r在这一步中，我们需要导入一些必要的库，这些库包括：\ncopy：用于复制对象 time：用于计算时间 torch：用于深度学习 torchvision：用于处理图像数据 numpy：用于数值计算 matplotlib：用于数据可视化 modle：用于导入模型 torch.nn：用于定义神经网络层 pandas：用于数据处理 import copy import time import torch from torchvision.datasets import FashionMNIST from torchvision import transforms import torch.utils.data as Data import numpy as np import matplotlib.pyplot as plt from modle import AlexNet import torch.nn as nn import pandas as pd 2. 数据处理\rdef train_val_data_process(): # 从FashionMNIST中导入训练数据，将数据转换为227*227的大小，同时将数据转换为tensor类型 train_data = FashionMNIST(root=\u0026amp;#39;./data\u0026amp;#39;, train=True, transform=transforms.Compose([transforms.Resize(size=227), transforms.ToTensor()]), download=True) # 划分训练集和验证集，训练集占80%，验证集占20% train_data, val_data = Data.random_split(train_data, [round(0.8*len(train_data)), round(0.2*len(train_data))]) # 划分训练集数据的批次大小，每个批次32个样本 train_dataloader = …","date": "2025-11-07 15:00:00",
    "updated": "2025-11-07 15:00:00"
  }, 
  {
    "objectID": "6e41b93accb5d6faf5456a18f36f45c2689f1da9",
    "permalink": "/post/no.8/",
    "title": "AlexNet原理","content": "\r1. 背景\rAlexNet 是 2012 年由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 提出的卷积神经网络架构。它在 ImageNet 图像分类任务上取得了显著的成果，成为了卷积神经网络的一个重要里程碑。 它在当年的 ImageNet 图像分类任务上，取得第一名的成绩，也向世界证明了卷积神经网络在图像识别任务上要优于传统的机器学习算法。\n2. AlexNet 网络结构\r3. 网络参数详解\r4. Dropout 层\r为什么要加入 Dropout 层？\n其实是为了降低网络的参数，由于层数的增加，网络的参数也会增加，而 Dropout 层可以随机的将一些神经元的输出设为 0，从而减少网络的参数。 加快了网络的训练速度，同时也提高了网络的泛化能力。\n5. 图像增强\r5.1 水平翻转\r将图像左右翻转，增加数据集的多样性。\n5.2 随机裁剪\r从图像中随机裁剪出一个子区域，增加数据集的多样性。\n5.3 PCA\rPCA：主成分分析，将图像的像素值转换为主成分，从而减少图像的维度。\n5.4 LRN正则化\rLRN 正则化：局部响应归一化，对每个像素的响应进行归一化，从而提高模型的泛化能力。\n总的来说，图像增强技术可以增加数据集的多样性，提高模型的泛化能力。\n6. 总结\r","date": "2025-11-06 15:00:00",
    "updated": "2025-11-06 15:00:00"
  }, 
  {
    "objectID": "d5bdae485b1876705d0acd377b12e3d7d9cdd7e6",
    "permalink": "/post/no.7/",
    "title": "LeNet原理","content": "\r1. LeNet-5 诞生背景\rLenet-5 是 Yann LeCun 于 1998 年提出的卷积神经网络（Convolutional Neural Network，CNN）架构，用于识别手写数字。\n2. LeNet-5 网络结构\rLeNet-5 由 7 层组成，包括 2 个卷积层、2 个池化层、3 个全连接层。\nLeNet-5 后面的尾标5其实指的是它有2个卷积层，3个全连接层。池化层并不算在其中，包括后面的模型也是如此，尾标只包括卷积层和全连接层的数量。\n3. LeNet-5 网络层详解\r3.1 LeNet-5 网络参数详解\r4. 总结\r","date": "2025-11-06 15:00:00",
    "updated": "2025-11-06 15:00:00"
  }, 
  {
    "objectID": "60b6657f0eb1a28c32117a58cebecdae75db68c6",
    "permalink": "/post/no.6/",
    "title": "CNN卷积神经网络算法原理 第三章","content": "\r图片在计算机中的本质\r单色图片\r彩色图片\r其实从图片中我们可以看出，图片在计算机中的本质就是一个二维数组，每个元素就是一个像素点，每个像素点有三个通道，分别是RGB通道，每个通道的值范围是0-255，0表示该通道的亮度最低，255表示该通道的亮度最高。\n全连接神经网络在图像识别中的问题\r从全连接神经网络的结构我们不难看出，图片是一个二维数组，而全连接神经网络的输入层是一个一维数组，所以我们需要将图片展开成一个一维数组，才能输入到全连接神经网络中。 但是，将图片展开成一个一维数组会导致一个问题，就是图片的空间信息会丢失。\n例如，一张图片是一个28x28的灰度图片，展开成一个一维数组就是784个元素，每个元素就是一个像素点的灰度值。 但是，我们可以看出，图片中存在着空间信息，例如，一个像素点的灰度值和它的邻居像素点的灰度值是相关的。\n如果我们将图片展开成一个一维数组，那么这些空间信息就会丢失，模型就无法利用这些空间信息来进行识别。 为了解决上面的问题，我们需要引入卷积这个操作来提取图片的空间信息，以保留图片的空间结构特征。\n卷积层\r卷积运算\r不加偏置项的卷积运算\r卷积运算的过程就是将卷积核在图片上滑动，每次滑动一个像素，计算卷积核和图片上对应位置的元素的乘积，然后将这些乘积相加，得到卷积结果。 总结就一句话，先相乘，在相加\n加入偏置项的卷积运算\r加入偏置项的卷积运算就是在卷积运算的基础上，加上一个偏置项b，最终的卷积结果就是卷积核和图片上对应位置的元素的乘积的和再加上偏置项b。\n填充\r填充就是在图片的边界上添加一些像素点，以保持卷积后的图片大小不变。\n步幅\r步幅就是卷积核在图片上滑动的步长，步幅越大，卷积后的图片越小。\n卷积公式\r卷积公式就是根据卷积核的大小、填充、步幅等参数，计算出卷积后的图片大小。\n多通道卷积\r多通道卷积就是在每个通道上进行卷积运算，最后将所有通道的卷积结果合并起来。\n利用立体图来表示卷积\r池化\r最大池化\r最大池化就是在每个池化窗口中，取窗口内的最大值作为池化结果。\n平均池化\r平均池化就是在每个池化窗口中，取窗口内的平均值作为池化结果。\n其实池化后的特征图大小的计算公式跟卷积是一样的，是通用的而且及其重要。\n卷积神经网络的整体结构\r总结\r卷积神经网络其实就是在全连接神经网络的基础上迭代而来的，它解决了全连接神经网络在图像识别中的问题，即图片的空间信 …","date": "2025-11-05 15:00:00",
    "updated": "2025-11-05 15:00:00"
  }, 
  {
    "objectID": "f39e7c3061801b6c2e3b38f34df2fecb6b257190",
    "permalink": "/post/no.5/",
    "title": "CNN卷积神经网络算法原理 第二章","content": "\r前向传播\r如下图所示，这就是神经网络前向传播的计算过程\n而下图所示，是一个具体的前向传播计算过程\n损失函数\r在神经网络中我们一般使用的是均方误差损失函数，公式如下：\n反向传播\r如下图所示，这就是神经网络反向传播的一个案例\n通常反向传播的计算过程如下： 1. 求参数的梯度 2. 利用参数的梯度进行更新 3. 新参数的前向传播\n下面我们就根据一个简单的案例来具体演示一下反向传播的过程。\n求参数的梯度\r利用参数的梯度进行更新\r新参数的前向传播\r总结\r神经网络反向传播的过程就是根据损失函数对参数的梯度，利用梯度下降法对参数进行更新，运用新的参数进行前向传播，从而不断的去优化模型，直到模型的损失函数收敛到一个最优解。\n","date": "2025-11-04 15:00:00",
    "updated": "2025-11-04 15:00:00"
  }, 
  {
    "objectID": "58cdda505b18fd1ab2ebbf7e0c0fe01430bbdaa5",
    "permalink": "/post/no.4/",
    "title": "CNN卷积神经网络算法原理 第一章","content": "\r全连接神经网络整体结构\r全连接神经网络的整体结构如下：\n上述图片就是一个全连接神经网络的整体结构，它基本上有以下几个部分：\n输入层：输入层主要是接收外部输入的数据 隐藏层：隐藏层可以有多个，每个隐藏层由多个神经元组成，每个神经元接收来自上一层的所有节点的输入 输出层：输出层的节点数等于分类的类别数，每个节点对应一个类别。 全连接神经网络的单元结构\r每个神经元的计算过程如下：\n由图中不难看出，它的计算过程神似我们大脑中的神经元结构，这也就是为啥要叫全连接神经网络。 全连接神经网络与卷积神经网络的区别就是其中的神经元，其中CNN卷积神经网络的神经元其实就是一个个卷积核，将全连接神经网络中的每个神经元都替换为卷积核，就得到了卷积神经网络。\n激活函数\r激活函数的作用就是将神经元的输出映射到一个指定的范围，常用的激活函数有sigmoid函数、tanh函数、ReLU函数等。\n为啥要加入激活函数呢？ 其一因为如果不加入激活函数，那么神经网络就变成了一个线性模型，无法解决非线性问题。 其二就是加入激活函数可以使神经网络的输出在一个指定的范围内，这对于分类问题尤为重要。 其三就是加入激活函数可以使神经网络的训练更加高效，因为激活函数的导数在大部分情况下都是一个常数，这就使得反向传播算法的计算更加简单。\nsigmoid函数\rsigmoid函数的图像如下：\ntanh函数\rtanh函数的图像如下：\nReLU函数\rReLU函数的图像如下：\nLeaky ReLU函数\rLeaky ReLU函数的图像如下：\n总结\r这一章简单介绍了一下全连接神经网络的整体结构、单元结构、激活函数，以及它们的作用。\n","date": "2025-11-03 15:00:00",
    "updated": "2025-11-03 15:00:00"
  }, 
  {
    "objectID": "95515b1ac18353ab95200c470beac299822046d0",
    "permalink": "/post/no.3/",
    "title": "线性回归模型案例的代码复现","content": "\r案例\r上述就是我们要解决的问题，即通过线性回归模型来对未知的y值进行预测。下面我们将通过代码复现一下这个案例。\n代码复现\r# 定义数据集 # 定义数据特征 x_data = [1, 2, 3] # 定义数据标签 y_data = [2, 4, 6] # 初始化参数W w = 4 #定义线性回归模型 def forword(x): return x * w #定义损失函数 def cost(xs, ys): costvalue = 0 for x , y in zip(xs, ys): y_pred = forword(x) costvalue += (y_pred - y)**2 return costvalue / len(xs) #定义计算梯度的函数 def gradient(xs, ys): grad = 0 for x, y in zip(xs, ys): grad += 2 * x * (x * w - y) return grad / len(xs) for epoch in range(100): #计算误差损失 cost_val = cost(x_data, y_data) grad_val = gradient(x_data, y_data) w =w-0.01 * grad_val print(\u0026#39;训练轮次：\u0026#39;, epoch, \u0026#34;w=\u0026#34;, w,\u0026#34;loss\u0026#34;, cost_val) 结果\r运行上述的代码，我们不难得出当w接近2时，损失函数也接近0，这也验证了我们的模型是正确的。\n","date": "2025-11-02 15:00:00",
    "updated": "2025-11-02 15:00:00"
  }, 
  {
    "objectID": "4981202e06eb49e7473b6db82030b5bac0fff769",
    "permalink": "/post/no.2/",
    "title": "梯度更新","content": "\r梯度下降法\r根据上一篇的文章我们知道，我们可以用最小二乘法根据损失函数与参数w来求w的最优解。具体该怎么做呢？我们可以联想到以下的场景：\n我们根据以上场景，再结合损失函数与参数w的关系，我们可以得到下图中的公式\n该公式就是梯度下降法的公式，但其中具体是怎么来到这就可以参考高中数学的微积分中的微分原理。\n上图就是一个简单的微分原理演示图，其中我们不难看出梯度下降法中的a可以类比为微分中的∆Xm，而我们该如何求出这一段X值所对应的y值呢，其实很简单，我们拿最近简单一阶方程y=kx举例，我们对X进行求导，就可以得到k的值，而对应的y值就是k*Xm。 而在梯度下降法中，在我们给定一个初始值w0后，我们就可以根据梯度下降法的公式，不断的去更新w0，直到w0收敛到一个最优解。也就是图像中的最低点所以在梯度下降法中，学习率a对模型的影响极大，它不仅影响了模型的收敛速度，还影响了模型的精度；当我们设置a时要慎重考虑。\n案例求解\r下面我们根据以上梯度下降法的求解过程，对上一篇文章中的案例进行求解 参考以下的案例，我们不难求解出上一篇文章中的案例，以下的图片就是求解过程 总结\r根据上述论断我们不难看出梯度下降法实际上就是根据微分的思想，将我们寻找损失函数的最小值的过程，在a的控制下，进行不断的细化拆分，直到我们找到一个最优解。\n","date": "2025-11-01 15:00:00",
    "updated": "2025-11-01 15:00:00"
  }, 
  {
    "objectID": "9bc61343927d1df4cc49649fdcf243c9cb58447f",
    "permalink": "/post/no.1/",
    "title": "线性回归原理详解","content": "\r线性回归模型\r参考人类的学习方式，我们一般学会识别一个物体，都是通过观察大量的样本，然后根据样本中的特征进行识别的；同理机器学习也是如此，其中我们将事物的名称命名为标签及为y或，而特征则为x。机器学习就是通过学习大量的样本，然后根据样本中的标签去获得样品的特征，从而对新的样本进行预测。 就像我们如何判断一个人是否为男性，我们一般会根据他的特征来判断，比如他是否有长的头发、是否有喉结等；而机器学习就是通过学习大量的样本，然后根据样本中的特征去判断新的样本是否为男性。 所以线性回归模型就可以简化为y关于x的求解，就是已知x，然后根据x去预测y。而机器学习的目的就是确定x的系数，从而对新的样本进行预测。\n案例\r如同以上案例，我们希望通过已有的学习时间，去预测未知学习时间的考试分数。\n误差函数\r参考上述的案例，我们根据学习时间和考试分数的落点分布，设线性回归函数，同时为了避免我们获得的结果与现实的结果出现很大的偏差，我们需要引入误差函数。误差函数的作用就是衡量我们获得的结果与现实的结果之间的差异，从而判断我们的解是否准确。下面我们就对该回归函数求解\n穷举法\r穷举法是一种简单的求解方法，它的基本思想是：在所有可能的解中，选择一个最优解。但当我们的解空间很大时，穷举法的效率就会很低。所以我们引入了其他的求解方法，那个方法就是最小二乘法\n最小二乘法\r我们根据穷举法可以画出误差函数的图像，而最小二乘法的基本思想就是：在所有可能的解中，选择一个使得误差函数最小的解。以这个解为基础，我们可以得到线性回归函数的系数。\n求解\r下面我们根据以上最小二乘法的求解过程，对线性回归模型进行求解\n","date": "2025-10-30 15:00:00",
    "updated": "2025-10-30 15:00:00"
  }]