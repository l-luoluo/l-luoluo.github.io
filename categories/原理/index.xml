<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>原理 on 心有所向，日复一日，必有精进</title>
    <link>http://localhost:1313/categories/%E5%8E%9F%E7%90%86/</link>
    <description>Recent content from 心有所向，日复一日，必有精进</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    
    <managingEditor>xxx@example.com (落-luo)</managingEditor>
    <webMaster>xxx@example.com (落-luo)</webMaster>
    
    <copyright>本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</copyright>
    
    <lastBuildDate>Mon, 24 Nov 2025 15:00:00 +0800</lastBuildDate>
    
    
    <atom:link href="http://localhost:1313/categories/%E5%8E%9F%E7%90%86/index.xml" rel="self" type="application/rss&#43;xml" />
    

    
    

    <item>
      <title>感知机</title>
      <link>http://localhost:1313/post/no.36/</link>
      <pubDate>Mon, 24 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.36/</guid>
      <description>
        <![CDATA[<h1>感知机</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><ul>
<li><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_12.pdf">感知机</a></li>
</ul>
<h3 id="1感知机模型">
<a class="header-anchor" href="#1%e6%84%9f%e7%9f%a5%e6%9c%ba%e6%a8%a1%e5%9e%8b"></a>
1.感知机模型
</h3><ul>
<li>感知机简单来说就是一个线性分类模型，其输入为实例的特征向量，输出为实例的类别。</li>
<li>在神经网络中，感知机模型被用作基础的神经元模型，用于构建更复杂的神经网络。</li>
<li>它通常也被叫做隐藏层神经元</li>
</ul>
<p><img src="/img/NO.36/%E5%9B%BE_01.png" alt=""></p>
        
        <hr><p>本文2025-11-24首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-24</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>softmax回归</title>
      <link>http://localhost:1313/post/no.34/</link>
      <pubDate>Sun, 23 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.34/</guid>
      <description>
        <![CDATA[<h1>softmax回归</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_10.pdf">softmax回归</a></p>
<h4 id="1softmax回归诞生">
<a class="header-anchor" href="#1softmax%e5%9b%9e%e5%bd%92%e8%af%9e%e7%94%9f"></a>
1.softmax回归诞生
</h4><ul>
<li>softmax回归是一种用于多分类问题的线性模型。</li>
<li>在实际应用中，我们通常会遇到多分类问题，例如识别图像中的物体、分类文本等。</li>
<li>但线性回归只能处理二分类问题，不能直接处理多分类问题。</li>
<li>因此，我们需要创造出一个新的模型，能够处理多分类问题。</li>
<li>softmax回归就是为了解决这个问题而诞生的。</li>
<li>它的基本思想是将每个类别的输出转换为一个概率值，然后选择概率最高的类别作为预测结果。</li>
<li>其本质还是一个线性模型，只是在输出层添加了一个softmax函数，将线性输出转换为概率分布。</li>
</ul>
<p><img src="/img/NO.34/%E5%9B%BE_01.png" alt=""></p>
        
        <hr><p>本文2025-11-23首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-23</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>基础优化方法</title>
      <link>http://localhost:1313/post/no.32/</link>
      <pubDate>Fri, 21 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.32/</guid>
      <description>
        <![CDATA[<h1>基础优化方法</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_9.pdf">基础优化方法</a></p>
<h3 id="1梯度下降">
<a class="header-anchor" href="#1%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d"></a>
1.梯度下降
</h3><p>梯度下降是一种优化算法，用于最小化损失函数。它的基本思想是通过迭代更新参数，使损失函数达到最小值。</p>
<p>梯度下降的更新规则如下：
</p>
        
        <hr><p>本文2025-11-21首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-21</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>线性回归</title>
      <link>http://localhost:1313/post/no.30/</link>
      <pubDate>Thu, 20 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.30/</guid>
      <description>
        <![CDATA[<h1>线性回归</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_8.pdf">线性回归</a></p>
<h4 id="线性模型">
<a class="header-anchor" href="#%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b"></a>
线性模型
</h4><p><img src="/img/NO.30/%E5%9B%BE_01.png" alt=""></p>
<p><img src="/img/NO.30/%E5%9B%BE_02.png" alt=""></p>
<p><img src="/img/NO.30/%E5%9B%BE_03.png" alt=""></p>
<h4 id="损失函数">
<a class="header-anchor" href="#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0"></a>
损失函数
</h4><p>为了衡量模型的预测值与真实值之间的差异，我们引入了损失函数。
损失函数是一个标量值，用于衡量模型的预测值与真实值之间的差异。</p>
        
        <hr><p>本文2025-11-20首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-20</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>自动求导</title>
      <link>http://localhost:1313/post/no.28/</link>
      <pubDate>Wed, 19 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.28/</guid>
      <description>
        <![CDATA[<h1>自动求导</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_7.pdf">自动求导</a></p>
<h4 id="1-向量的链式法则">
<a class="header-anchor" href="#1-%e5%90%91%e9%87%8f%e7%9a%84%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99"></a>
1. 向量的链式法则
</h4><p>向量的链式法则是标量的链式法则在向量上的推广。
它的基本思想是：如果一个函数 $f$ 是向量的函数，而 $g$ 是标量的函数，那么 $f$ 对 $g$ 的导数就是 $f$ 对每个元素的导数与 $g$ 对每个元素的导数的点积。</p>
        
        <hr><p>本文2025-11-19首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-19</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>矩阵计算</title>
      <link>http://localhost:1313/post/no.27/</link>
      <pubDate>Tue, 18 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.27/</guid>
      <description>
        <![CDATA[<h1>矩阵计算</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_6.pdf">矩阵计算</a></p>
<h4 id="1-标量导数">
<a class="header-anchor" href="#1-%e6%a0%87%e9%87%8f%e5%af%bc%e6%95%b0"></a>
1. 标量导数
</h4><p>标量函数的导数是一个标量，它是切线的斜率，用于表示函数在某个点的变化率。</p>
<p><img src="/img/NO.27/%E5%9B%BE_01.png" alt=""></p>
<h4 id="2亚导数">
<a class="header-anchor" href="#2%e4%ba%9a%e5%af%bc%e6%95%b0"></a>
2.亚导数
</h4><p>亚导数是指在某个点上函数的导数不存在时，我们可以用一个小于导数的量来代替导数，这个量就叫做亚导数。
也就是将导数拓展到不可微的函数</p>
        
        <hr><p>本文2025-11-18首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-18</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>线性代数</title>
      <link>http://localhost:1313/post/no.25/</link>
      <pubDate>Mon, 17 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.25/</guid>
      <description>
        <![CDATA[<h1>线性代数</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_5.pdf">线性代数</a></p>
<h3 id="线性代数">
<a class="header-anchor" href="#%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0"></a>
线性代数
</h3><h4 id="1-标量与简单运算">
<a class="header-anchor" href="#1-%e6%a0%87%e9%87%8f%e4%b8%8e%e7%ae%80%e5%8d%95%e8%bf%90%e7%ae%97"></a>
1. 标量与简单运算
</h4><p>标量是一个只有大小没有方向的量。在数学中，标量通常用小写字母表示，例如 $a, b, c$ 等。</p>
<p><img src="/img/NO.25/%E5%9B%BE_01.png" alt=""></p>
<h4 id="2-向量与简单运算">
<a class="header-anchor" href="#2-%e5%90%91%e9%87%8f%e4%b8%8e%e7%ae%80%e5%8d%95%e8%bf%90%e7%ae%97"></a>
2. 向量与简单运算
</h4><p>向量是一个有序的量，它可以表示在空间中的一个点或方向。在数学中，向量通常用小写字母表示，例如 $x, y, z$ 等。
向量可以用箭头来表示，箭头的长度表示向量的大小，箭头的方向表示向量的方向。</p>
        
        <hr><p>本文2025-11-17首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-17</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>数据预处理</title>
      <link>http://localhost:1313/post/no.24/</link>
      <pubDate>Sun, 16 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.24/</guid>
      <description>
        <![CDATA[<h1>数据预处理</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/notebooks/chapter_preliminaries/pandas.slides.html#/">数据预处理</a></p>
<h3 id="数据预处理">
<a class="header-anchor" href="#%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86"></a>
数据预处理
</h3><h4 id="1创建一个简单的数据集">
<a class="header-anchor" href="#1%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aa%e7%ae%80%e5%8d%95%e7%9a%84%e6%95%b0%e6%8d%ae%e9%9b%86"></a>
1.创建一个简单的数据集
</h4><p><img src="/img/NO.24/%E5%9B%BE_01.png" alt=""></p>
<h4 id="2加载数据集">
<a class="header-anchor" href="#2%e5%8a%a0%e8%bd%bd%e6%95%b0%e6%8d%ae%e9%9b%86"></a>
2.加载数据集
</h4><p><img src="/img/NO.24/%E5%9B%BE_02.png" alt=""></p>
<h4 id="3数据预处理">
<a class="header-anchor" href="#3%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86"></a>
3.数据预处理
</h4><p>为了处理缺失的数据，我们一般会使用以下方法：</p>
<ol>
<li>删除缺失值</li>
<li>填充缺失值（插值）
在实际应用中，我们通常会使用均值、中位数或众数来填充缺失值。
所以下面是一个使用均值填充缺失值的示例：</li>
</ol>
<p><img src="/img/NO.24/%E5%9B%BE_03.png" alt=""></p>
        
        <hr><p>本文2025-11-16首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-16</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>数据操作</title>
      <link>http://localhost:1313/post/no.23/</link>
      <pubDate>Sat, 15 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.23/</guid>
      <description>
        <![CDATA[<h1>数据操作</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="链接">
<a class="header-anchor" href="#%e9%93%be%e6%8e%a5"></a>
链接
</h3><p><a href="https://courses.d2l.ai/zh-v2/assets/pdfs/part-0_4.pdf">数据操作</a></p>
<h3 id="n维数组样例">
<a class="header-anchor" href="#n%e7%bb%b4%e6%95%b0%e7%bb%84%e6%a0%b7%e4%be%8b"></a>
N维数组样例
</h3><p>一般情况下，我们会使用N维数组来表示一个样本，其中N为样本的维度。
N=0代表着样本的数量，N=1代表着样本的特征维度，N=2代表着样本的特征矩阵维度。
N=3代表着样本的特征矩阵的维度，N=4代表着样本的特征矩阵的维度的维度，以此类推。</p>
        
        <hr><p>本文2025-11-15首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-15</p>]]>
      </description>
      
        <category>跟着李沐学AI</category><category>动手学深度学V2</category><category>原理</category>
      
    </item>
    
    

    <item>
      <title>ResNet模型原理</title>
      <link>http://localhost:1313/post/no.21/</link>
      <pubDate>Fri, 14 Nov 2025 15:00:00 &#43;0800</pubDate>
      <author>xxx@example.com (落-luo)</author>
      <guid>http://localhost:1313/post/no.21/</guid>
      <description>
        <![CDATA[<h1>ResNet模型原理</h1><p>作者：落-luo（xxx@example.com）</p>
        
          <h3 id="resnet诞生背景">
<a class="header-anchor" href="#resnet%e8%af%9e%e7%94%9f%e8%83%8c%e6%99%af"></a>
ResNet诞生背景
</h3><p>ResNet是2015年由微软的Kaiming He等人提出的，其主要目标是解决深度神经网络训练时的梯度消失问题。
同时，ResNet还提出了一种新的网络结构，即残差块（Residual Block），用于解决深度神经网络训练时的梯度消失问题。
后来，残差块被广泛应用于深度神经网络中，成为了一种非常重要的网络结构。</p>
        
        <hr><p>本文2025-11-14首发于<a href='http://localhost:1313/'>心有所向，日复一日，必有精进</a>，最后修改于2025-11-14</p>]]>
      </description>
      
        <category>炮哥带你学</category><category>pytorch与经典卷积神经网络</category><category>原理</category>
      
    </item>
    
  </channel>
</rss>
